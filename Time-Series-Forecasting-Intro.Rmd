---
title: "Introduction to Time Series Forcasting"
author: "Mohammed Ali"
date: "March 2, 2018"
output: html_document
---

# ARIMA Cheat Sheet

* Examine your data
    + Plot the data and examine its patterns and irregularities.
    + Clean up any outliers or missing values if needed.
    + `tsclean()` is a convenient method for outlier removal and inputting missing values.
    + Take a logarithm of a series to help stabilize a strong growth trend.

* Decompose your data
    + Does the series appear to have trends or seasonality?
    + Use `decompose()` or `stl()` to examine and possibly remove components of the series.

* Stationarity
    + Is the series stationary?
    + Use `adf.test()`, `ACF`, `PACF` plots to determine order of differencing needed.

* Autocorrelations and choosing model order
    + Choose order of the _ARIMA_ by examining `ACF` and `PACF` plots.

* Fit an ARIMA model

* Evaluate and iterate
    + Check residuals, which should haven no patterns and be normally distributed.
    + If there are visible patterns or bias, plot `ACF/PACF`. Are any additional order parameters needed?
    + Refit model if needed. Compare model errors and fit criteria such as `AIC` or `BIC`.
    + Calculate forecast using the chosen model.

# Setup
## Load R Packages

```{r Load_R_Packages}
library(tidyverse)
library(tseries)
library(forecast)
```

## Load DataSet
his dataset contains the hourly and daily count of rental bikes between years 2011 and 2012 in Capital bikeshare system with the corresponding weather and seasonal information.


```{r load_dataset}
daily_data = read_csv('data/day.csv')
```


# Data EDA
A good starting point is to plot the series and visually examine it for any outliers, volatility, or irregularities. 

In this case, bicycle checkouts are showing a lot of fluctuations from one day to another. However, even with this volatility present, we already see some patterns emerge. For example, lower usage of bicycles occurs in the winter months and higher checkout numbers are observed in the summer months:
```{r data_eda}
daily_data$Date = as.Date(daily_data$dteday)
ggplot(daily_data, aes(Date, cnt)) + geom_line() + scale_x_date('month')  + ylab("Daily Bike Checkouts") +
            xlab("")
```

In some cases, the number of bicycles checked out dropped below 100 one day and rose to over 4,000 the next day. These are suspected outliers that could bias the model by skewing statistical summaries. R provides a convenient method for removing time series outliers: tsclean() as part of its forecast package. tsclean() identifies and replaces outliers using series smoothing and decomposition. This method is also capable of inputting missing values in the series if there are any.

Note that we are using the ts() command to create a time series object to pass to tsclean():

```{r ts_clean}
count_ts = ts(daily_data[, c('cnt')])

daily_data$clean_cnt <- tsclean(count_ts[1:length(count_ts),])

ggplot() +
  geom_line(data = daily_data, aes(x = Date, y = clean_cnt)) + ylab('Cleaned Bicycle Count')
```

Even after removing outliers, the daily data is still pretty volatile. Let us smooth the draw

```{r smooth}
# Weekly
daily_data$cnt_ma <- ma(daily_data$clean_cnt, order=7) # using the clean count with no outliers
# Monthly
daily_data$cnt_ma30 <- ma(daily_data$clean_cnt, order=30)


ggplot() +
  geom_line(data = daily_data, aes(x = Date, y = clean_cnt, colour = "Counts")) +
  geom_line(data = daily_data, aes(x = Date, y = cnt_ma,   colour = "Weekly Moving Average"))  +
  geom_line(data = daily_data, aes(x = Date, y = cnt_ma30, colour = "Monthly Moving Average"))  +
  ylab('Bicycle Count')
```

In addition to volatility, modeling daily level data might require specifying multiple seasonality levels, such day of the week, week of the year, month of the year, holidays, etc. For the sake of simplicity, we will model the smoothed series of weekly moving average (as shown by the blue line above).


# Decompose Data
First, we calculate the seasonal component of the data using `stl()`. STL is a flexible function for decomposing and forecasting the series. It calculates the seasonal component of the series using smoothing, and adjusts the original series by subtracting seasonality in two simple lines:
```{r decompose_Season}
count_ma <- ts(na.omit(daily_data$cnt_ma), frequency=30)
decomp <- stl(count_ma, s.window="periodic")
deseasonal_cnt <- seasadj(decomp)
plot(decomp)
```

# Ensuring Stationarity
Test stationarity
```{r test_stationarity}
adf.test(count_ma, alternative = "stationary")
```


# Autocorrelations and Choosing Model Order

```{r ACF}
Acf(count_ma, main='')

Pacf(count_ma, main='')
```

We can start with the order of d = 1 and re-evaluate whether further differencing is needed.

The augmented Dickey-Fuller test on differenced data rejects the null hypotheses of non-stationarity. Plotting the differenced series, we see an oscillating pattern around 0 with no visible strong trend. This suggests that differencing of order 1 terms is sufficient and should be included in the model. 

```{r diff_1}
count_d1 <- diff(deseasonal_cnt, differences = 1)
plot(count_d1)
adf.test(count_d1, alternative = "stationary")
```


Next, spikes at particular lags of the differenced series can help inform the choice of p or q for our model:

```{r acf_diff}
Acf(count_d1, main='ACF for Differenced Series')
Pacf(count_d1, main='PACF for Differenced Series')
```





# Fitting an ARIMA model
We can specify non-seasonal ARIMA structure and fit the model to de-seasonalize data. Parameters (1,1,1) suggested by the automated procedure are in line with our expectations based on the steps above; the model incorporates differencing of degree 1, and uses an autoregressive term of first lag and a moving average model of order 1:
```{r auto}
auto.arima(deseasonal_cnt, seasonal=FALSE)
```

# Evaluate and Iterate
```{r acf_auto}
fit<-auto.arima(deseasonal_cnt, seasonal=FALSE)
tsdisplay(residuals(fit), lag.max=45, main='(1,1,1) Model Residuals')
```



There is a clear pattern present in ACF/PACF and model residuals plots repeating at lag 7. This suggests that our model may be better off with a different specification, such as p = 7 or q = 7. 

```{r arima_7}
fit2 <- arima(deseasonal_cnt, order=c(1,1,7))

fit2

tsdisplay(residuals(fit2), lag.max=15, main='Seasonal Model Residuals')
```

Forecasting using a fitted model is straightforward in R. We can specify forecast horizon h periods ahead for predictions to be made, and use the fitted model to generate those predictions:

```{r forcast}
fcast <- forecast(fit2, h=30)
plot(fcast)
```
The light blue line above shows the fit provided by the model, but what if we wanted to get a sense of how the model will perform in the future? One method is to reserve a portion of our data as a "hold-out" set, fit the model, and then compare the forecast to the actual observed values:

```{r evaluate}
hold <- window(ts(deseasonal_cnt), start=700)

fit_no_holdout = arima(ts(deseasonal_cnt[-c(700:725)]), order=c(1,1,7))

fcast_no_holdout <- forecast(fit_no_holdout,h=25)
plot(fcast_no_holdout, main=" ")
lines(ts(deseasonal_cnt))
```


However, the blue line representing forecast seems very naive: It goes close to a straight line fairly soon, which seems unlikely given past behavior of the series. Recall that the model is assuming a series with no seasonality, and is differencing the original non-stationary data. In other words, plotted predictions are based on the assumption that there will be no other seasonal fluctuations in the data and the change in the number of bicycles from one day to another is more or less constant in mean and variance. This forecast may be a naive model, but it illustrates the process of choosing an ARIMA model and could also serve as a benchmark to grade against as more complex models are built.

How can we improve the forecast and iterate on this model? One simple change is to add back the seasonal component we extracted earlier. Another approach would be to allow for (P, D, Q) components to be included in the model, which is a default in the auto.arima() function. Re-fitting the model on the same data, we see that there still might be some seasonal pattern in the series, with the seasonal component described by AR(1): 

```{r model_2}
fit_w_seasonality = auto.arima(deseasonal_cnt, seasonal=TRUE)
fit_w_seasonality
seas_fcast <- forecast(fit_w_seasonality, h=30)
plot(seas_fcast)
```


# Credit
https://www.datascience.com/learn-data-science/tutorials/introduction-to-forecasting-with-arima-r-data-science


