---
title: "Introduction to Time Series Forcasting"
author: "Mohammed Ali"
date: "March 2, 2018"
output: html_document
---

# ARIMA Cheat Sheet

* Examine your data
    + Plot the data and examine its patterns and irregularities.
    + Clean up any outliers or missing values if needed.
    + `tsclean()` is a convenient method for outlier removal and inputting missing values.
    + Take a logarithm of a series to help stabilize a strong growth trend.

* Decompose your data
    + Does the series appear to have trends or seasonality?
    + Use `decompose()` or `stl()` to examine and possibly remove components of the series.

* Stationarity
    + Is the series stationary?
    + Use `adf.test()`, `ACF`, `PACF` plots to determine order of differencing needed.

* Autocorrelations and choosing model order
    + Choose order of the _ARIMA_ by examining `ACF` and `PACF` plots.

* Fit an ARIMA model

* Evaluate and iterate
    + Check residuals, which should haven no patterns and be normally distributed.
    + If there are visible patterns or bias, plot `ACF/PACF`. Are any additional order parameters needed?
    + Refit model if needed. Compare model errors and fit criteria such as `AIC` or `BIC`.
    + Calculate forecast using the chosen model.

# Setup
## Load R Packages

```{r Load_R_Packages}
library(tidyverse)
library(tseries)
library(forecast)
```

## Load DataSet
his dataset contains the hourly and daily count of rental bikes between years 2011 and 2012 in Capital bikeshare system with the corresponding weather and seasonal information.


```{r load_dataset}
daily_data = read_csv('data/day.csv')
```


# Data EDA
A good starting point is to plot the series and visually examine it for any outliers, volatility, or irregularities. 

In this case, bicycle checkouts are showing a lot of fluctuations from one day to another. However, even with this volatility present, we already see some patterns emerge. For example, lower usage of bicycles occurs in the winter months and higher checkout numbers are observed in the summer months:
```{r data_eda}
daily_data$Date = as.Date(daily_data$dteday)
ggplot(daily_data, aes(Date, cnt)) + geom_line() + scale_x_date('month')  + ylab("Daily Bike Checkouts") +
            xlab("")
```

In some cases, the number of bicycles checked out dropped below 100 one day and rose to over 4,000 the next day. These are suspected outliers that could bias the model by skewing statistical summaries. R provides a convenient method for removing time series outliers: tsclean() as part of its forecast package. tsclean() identifies and replaces outliers using series smoothing and decomposition. This method is also capable of inputting missing values in the series if there are any.

Note that we are using the ts() command to create a time series object to pass to tsclean():

```{r ts_clean}
count_ts = ts(daily_data[, c('cnt')])

daily_data$clean_cnt <- tsclean(count_ts[1:length(count_ts),])

ggplot() +
  geom_line(data = daily_data, aes(x = Date, y = clean_cnt)) + ylab('Cleaned Bicycle Count')
```

Even after removing outliers, the daily data is still pretty volatile. Let us smooth the draw

```{r smooth}
# Weekly
daily_data$cnt_ma <- ma(daily_data$clean_cnt, order=7) # using the clean count with no outliers
# Monthly
daily_data$cnt_ma30 <- ma(daily_data$clean_cnt, order=30)


ggplot() +
  geom_line(data = daily_data, aes(x = Date, y = clean_cnt, colour = "Counts")) +
  geom_line(data = daily_data, aes(x = Date, y = cnt_ma,   colour = "Weekly Moving Average"))  +
  geom_line(data = daily_data, aes(x = Date, y = cnt_ma30, colour = "Monthly Moving Average"))  +
  ylab('Bicycle Count')
```

In addition to volatility, modeling daily level data might require specifying multiple seasonality levels, such day of the week, week of the year, month of the year, holidays, etc. For the sake of simplicity, we will model the smoothed series of weekly moving average (as shown by the blue line above).


# Decompose Data
First, we calculate the seasonal component of the data using `stl()`. STL is a flexible function for decomposing and forecasting the series. It calculates the seasonal component of the series using smoothing, and adjusts the original series by subtracting seasonality in two simple lines:
```{r decompose_Season}
count_ma <- ts(na.omit(daily_data$cnt_ma), frequency=30)
decomp <- stl(count_ma, s.window="periodic")
deseasonal_cnt <- seasadj(decomp)
plot(decomp)
```

# Ensuring Stationarity
Test stationarity
```{r test_stationarity}
adf.test(count_ma, alternative = "stationary")
```


# Autocorrelations and Choosing Model Order

```{r ACF}
Acf(count_ma, main='')

Pacf(count_ma, main='')
```

We can start with the order of d = 1 and re-evaluate whether further differencing is needed.

The augmented Dickey-Fuller test on differenced data rejects the null hypotheses of non-stationarity. Plotting the differenced series, we see an oscillating pattern around 0 with no visible strong trend. This suggests that differencing of order 1 terms is sufficient and should be included in the model. 

```{r diff_1}
count_d1 <- diff(deseasonal_cnt, differences = 1)
plot(count_d1)
adf.test(count_d1, alternative = "stationary")
```


Next, spikes at particular lags of the differenced series can help inform the choice of p or q for our model:

```{r acf_diff}
Acf(count_d1, main='ACF for Differenced Series')
Pacf(count_d1, main='PACF for Differenced Series')
```





# Fitting an ARIMA model
We can specify non-seasonal ARIMA structure and fit the model to de-seasonalize data. Parameters (1,1,1) suggested by the automated procedure are in line with our expectations based on the steps above; the model incorporates differencing of degree 1, and uses an autoregressive term of first lag and a moving average model of order 1:
```{r auto}
auto.arima(deseasonal_cnt, seasonal=FALSE)
```

